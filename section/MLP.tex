\section{Word Embedding and Sentiment Analysis}

\subsection*{Skip-gram Model (word2vec)}
\textbf{Preprocess}: Given window size $k$ and corpus size $C$, find all pairs with a focus word $w$ and a context word $c$ which locates in the window centered around the focus word. The complexity is $O(k\cdot C)$.\\
\textbf{Model}: $p(c\mid w)=\frac{\exp(e_{\text{wrd}}(w)\cdot e_{\text{ctx}}(c))}{Z(w)}$. Each word has two different embedding: $e_{\text{wrd}}$ and $e_{\text{ctx}}$, either throw away $e_{ctx}$ or concatenate them with $e_{\text{wrd}}$ to form embeddings of dimensionality 2d. Estimated via MLE.

\subsection*{Combined Bag Of Words (CBOW)}
$\mathbf{e}(\mathbf{x})=\sum_{w \in x} \mathbf{e}(w)$

\subsection*{Evaluation of embeddings}
Word similarity: compare embedding similarity of similar words. \\
Word analogies: king - queen = man - woman.

\subsection*{Pipeline of Sentiment Analysis}
\textbf{Goal}: Predict whether a sentence is positive or negative.
\begin{enumerate}
    \item Preprocess: tokenization, stemming, stop word removal, etc.
    \item Word embedding using some models.
    \item Sentence embedding by pooling word embeddings: sum, mean, max, etc.
    \item Run MLP on the sentence embedding which has a fixed size.
\end{enumerate}

\textbf{tf-idf}$=\operatorname{tf}(t, d) \times \operatorname{idf}(t, D)$ \\
\textbf{tf}: term frequency, how frequent is term $t$ in document $d$, \\ 
\textbf{idf}: inverse document frequency, log(how frequent is term $t$ in corpus $D$).
